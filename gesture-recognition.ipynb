{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import the neccessary libraries","metadata":{"id":"l3pAYizsKiWB"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom imageio import imread\nfrom skimage.transform import resize\nimport datetime\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation, Conv3D, MaxPooling3D, TimeDistributed, GRU, LSTM, Input\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.backend import clear_session\nimport random as rn\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"M9rABrX20KCb","execution":{"iopub.status.busy":"2021-11-11T06:04:28.609251Z","iopub.execute_input":"2021-11-11T06:04:28.609516Z","iopub.status.idle":"2021-11-11T06:04:28.617325Z","shell.execute_reply.started":"2021-11-11T06:04:28.609486Z","shell.execute_reply":"2021-11-11T06:04:28.616641Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Set the random seed so that the results don't vary drastically each time the code is run","metadata":{"id":"Gf2nNc9B0KCe"}},{"cell_type":"code","source":"np.random.seed(100)\nrn.seed(100)\ntf.random.set_seed(100)","metadata":{"id":"z4r-kByJ0KCg","execution":{"iopub.status.busy":"2021-11-11T06:04:28.619213Z","iopub.execute_input":"2021-11-11T06:04:28.619628Z","iopub.status.idle":"2021-11-11T06:04:28.628281Z","shell.execute_reply.started":"2021-11-11T06:04:28.619591Z","shell.execute_reply":"2021-11-11T06:04:28.627551Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Download and unzip the dataset","metadata":{"id":"rP64gRurMjFu"}},{"cell_type":"code","source":"!gdown https://drive.google.com/uc?id=1ehyrYBQ5rbQQe6yL4XbLWe3FMvuVUGiL\n!unzip ./Project_data.zip","metadata":{"id":"sQhBlFJXKTrt","outputId":"e1914d82-d00d-441a-c156-5fbc64a699fb","execution":{"iopub.status.busy":"2021-11-11T06:04:47.875232Z","iopub.execute_input":"2021-11-11T06:04:47.875565Z","iopub.status.idle":"2021-11-11T06:05:20.769003Z","shell.execute_reply.started":"2021-11-11T06:04:47.875522Z","shell.execute_reply":"2021-11-11T06:05:20.768162Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!rm ./Project_data.zip","metadata":{"id":"moRPnwvIhvzF","execution":{"iopub.status.busy":"2021-11-11T06:05:20.771990Z","iopub.execute_input":"2021-11-11T06:05:20.772236Z","iopub.status.idle":"2021-11-11T06:05:21.666850Z","shell.execute_reply.started":"2021-11-11T06:05:20.772208Z","shell.execute_reply":"2021-11-11T06:05:21.665842Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Folder names for the training and validation data are read into train_doc and val_doc respectively. Set the batch size such that GPU memory is utilized to its full capacity","metadata":{"id":"PnKyszBJ0KCi"}},{"cell_type":"code","source":"train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\nval_doc = np.random.permutation(open('./Project_data/val.csv').readlines())\nbatch_size = 50","metadata":{"id":"qmx8XjU10KCj","execution":{"iopub.status.busy":"2021-11-11T06:05:21.670325Z","iopub.execute_input":"2021-11-11T06:05:21.670573Z","iopub.status.idle":"2021-11-11T06:05:22.331388Z","shell.execute_reply.started":"2021-11-11T06:05:21.670542Z","shell.execute_reply":"2021-11-11T06:05:22.330628Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class_label = [folder.split(\";\")[2].strip(\"\\n\") for folder in train_doc]\npd.Series(class_label).value_counts()","metadata":{"id":"VYXWAl-K0KCk","outputId":"e4eaefd9-850e-4c52-bcf4-52d5452d10dc","execution":{"iopub.status.busy":"2021-11-11T06:05:22.334137Z","iopub.execute_input":"2021-11-11T06:05:22.334897Z","iopub.status.idle":"2021-11-11T06:05:25.981869Z","shell.execute_reply.started":"2021-11-11T06:05:22.334858Z","shell.execute_reply":"2021-11-11T06:05:25.981055Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"There is no class imbalance as the no of samples is almost the same for each class. Hence accuracy can used as a metric to evaluate the performance of the model","metadata":{"id":"ntJPnwGd0KCo"}},{"cell_type":"markdown","source":"## Generator\n\n*   Build a data generator that generates batches of videos of size = batch_size\n*   Since all images don't have the same shape, they need to be resized.\n*   Not all frames of each video need to be included in the training data\n*   The images need to be normalised before they are fed to the model\n\n\n\n ","metadata":{"id":"mCdvUeVZ0KCp"}},{"cell_type":"code","source":"img_idx = np.arange(0, 30, 2) # list of indices of the frames that you want to use from each video\ndef generator(source_path, folder_list, batch_size, ablation = None):\n    print( 'Source path = ', source_path, 'batch size =', batch_size)\n    while True:\n        t = np.random.permutation(folder_list) # shuffle the data for each epoch\n        if ablation is None :\n          num_batches = len(t) // batch_size # calculate the number of batches\n          for batch in range(num_batches): # iterate over the number of batches\n            batch_data = np.zeros((batch_size, len(img_idx), 120, 120, 3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size, 5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    image = resize(image, (120, 120, 3)) # resize each image\n\n                    batch_data[folder,idx,:,:,0] = (image[:,:,0] - np.mean(image[:,:,0])) #centering the image\n                    batch_data[folder,idx,:,:,1] = (image[:,:,1] - np.mean(image[:,:,1])) #centering the image\n                    batch_data[folder,idx,:,:,2] = (image[:,:,2] - np.mean(image[:,:,2])) #centering the image\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels \n          \n          # for the remaining sequences\n          if len(t) % batch_size != 0 : \n            rem_data = len(t) % batch_size\n            batch_data = np.zeros((rem_data, len(img_idx), 120, 120, 3))\n            batch_labels = np.zeros((rem_data, 5))\n            for folder in range(rem_data) :\n              imgs = os.listdir(source_path + '/' + t[folder + (num_batches * batch_size)].strip().split(\";\")[0])\n              for idx, item in enumerate(img_idx) :\n                image = imread(source_path+'/'+ t[folder + (num_batches * batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                image = resize(image, (120, 120, 3))\n                batch_data[folder,idx,:,:,0] = (image[:,:,0] - np.mean(image[:,:,0])) #centering the image\n                batch_data[folder,idx,:,:,1] = (image[:,:,1] - np.mean(image[:,:,1])) #centering the image\n                batch_data[folder,idx,:,:,2] = (image[:,:,2] - np.mean(image[:,:,2])) #centering the image\n              batch_labels[folder, int(t[folder + (num_batches * batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels     \n\n            \n        else :\n          num_batches = ablation // batch_size\n          for batch in range(num_batches) :\n            batch_data = np.zeros((batch_size, len(img_idx), 120, 120, 3))\n            batch_labels = np.zeros((batch_size, 5))\n            for folder in range(batch_size) :\n              imgs = os.listdir(source_path + '/' + t[folder + (batch*batch_size)].strip().split(\";\")[0])\n              for idx, item in enumerate(img_idx) :\n                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                image = resize(image, (120, 120, 3))\n                batch_data[folder,idx,:,:,0] = (image[:,:,0] - np.mean(image[:,:,0])) #centering the image\n                batch_data[folder,idx,:,:,1] = (image[:,:,1] - np.mean(image[:,:,1])) #centering the image\n                batch_data[folder,idx,:,:,2] = (image[:,:,2] - np.mean(image[:,:,2])) #centering the image\n              batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n          yield batch_data, batch_labels\n        \n          if ablation % batch_size != 0 :\n            rem_data = ablation % batch_size\n            batch_data = np.zeros((rem_data, len(img_idx), 120, 120, 3))\n            batch_labels = np.zeros((rem_data, 5))\n            for folder in range(rem_data) :\n              imgs = os.listdir(source_path + '/' + t[folder + (num_batches * batch_size)].strip().split(\";\")[0])\n              for idx, item in enumerate(img_idx) :\n                image = imread(source_path+'/'+ t[folder + (num_batches * batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                image = resize(image, (120, 120, 3))\n                batch_data[folder,idx,:,:,0] = (image[:,:,0] - np.mean(image[:,:,0])) #centering the image\n                batch_data[folder,idx,:,:,1] = (image[:,:,1] - np.mean(image[:,:,1])) #centering the image\n                batch_data[folder,idx,:,:,2] = (image[:,:,2] - np.mean(image[:,:,2])) #centering the image\n              batch_labels[folder, int(t[folder + (num_batches * batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels     \n        \n","metadata":{"id":"Mydxr0680KCq","execution":{"iopub.status.busy":"2021-11-11T06:05:25.984357Z","iopub.execute_input":"2021-11-11T06:05:25.984625Z","iopub.status.idle":"2021-11-11T06:05:26.852110Z","shell.execute_reply.started":"2021-11-11T06:05:25.984587Z","shell.execute_reply":"2021-11-11T06:05:26.851095Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_path = './Project_data/train'\nval_path = './Project_data/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)","metadata":{"id":"OG19sz8p0KC8","outputId":"dedaa15e-30b5-4a98-b2f9-25c49ca42527","execution":{"iopub.status.busy":"2021-11-11T06:05:26.853562Z","iopub.execute_input":"2021-11-11T06:05:26.853835Z","iopub.status.idle":"2021-11-11T06:05:27.055438Z","shell.execute_reply.started":"2021-11-11T06:05:26.853800Z","shell.execute_reply":"2021-11-11T06:05:27.054696Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Looking at a random frame from a random video generated by the generator","metadata":{"id":"9ec-4yyK0KC-"}},{"cell_type":"code","source":"batch_data, batch_labels = next(generator(train_path, train_doc, batch_size))\nplt.figure(figsize = (5, 5))\nvid_rand = np.random.randint(batch_size)\nframe_rand = np.random.randint(len(img_idx))\nplt.imshow(batch_data[vid_rand, frame_rand, :, :, :])\nplt.axis(\"off\")\nplt.show()","metadata":{"id":"jwnDxtUb0KDC","outputId":"a3228bce-7978-4812-fd9e-57f284a8ad93","execution":{"iopub.status.busy":"2021-11-11T06:05:27.057695Z","iopub.execute_input":"2021-11-11T06:05:27.058130Z","iopub.status.idle":"2021-11-11T06:05:37.139531Z","shell.execute_reply.started":"2021-11-11T06:05:27.058090Z","shell.execute_reply":"2021-11-11T06:05:37.138680Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Modelling\n### Design the model in such a way that its accuracy is high with least number of parameters, so that it can fit in the memory of the webcam and respond instantly in real time scenarios","metadata":{"id":"u0gWPPBT0KDE"}},{"cell_type":"markdown","source":"### Defining a function that trains a model\n\nIt does the following :\n\n*   Defines the optimiser\n*   Compiles the model\n*   Defines callbacks such as ModelCheckpoint for saving the best model at the end of every epoch and ReduceLROnPlateau reducing the learning rate once the val_loss plateaus\n*   Creates the generator objects for the training data and validation data\n*   Calls the fit() method of the keras model\n*   Plots the validation accuracy and training accuracy across epochs","metadata":{"id":"dedtn75wLZ4V"}},{"cell_type":"code","source":"def model_training(model, lr, patience, factor, num_epochs = 30) :\n  adam = optimizers.Adam(learning_rate = lr)#write your optimizer\n  model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n\n  train_generator = generator(train_path, train_doc, batch_size)\n  val_generator = generator(val_path, val_doc, batch_size)\n  \n  curr_dt_time = datetime.datetime.now()\n  model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \n  if not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \n  filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\n  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto',save_freq = 'epoch')\n  LR = ReduceLROnPlateau(patience = patience, factor = factor, verbose = 1) \n  callbacks_list = [checkpoint, LR]\n\n  if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\n  else:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\n  if (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\n  else:\n    validation_steps = (num_val_sequences//batch_size) + 1\n\n  history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=[callbacks_list], validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n  \n  plt.figure(figsize = (8, 8))\n  plt.plot(range(num_epochs), history.history[\"categorical_accuracy\"])\n  plt.plot(range(num_epochs), history.history[\"val_categorical_accuracy\"])\n  plt.legend(labels = [\"train\", \"validation\"])\n  plt.show()","metadata":{"id":"Bpd9G2POLUkf","execution":{"iopub.status.busy":"2021-11-11T06:05:37.144636Z","iopub.execute_input":"2021-11-11T06:05:37.146853Z","iopub.status.idle":"2021-11-11T06:05:37.163750Z","shell.execute_reply.started":"2021-11-11T06:05:37.146805Z","shell.execute_reply":"2021-11-11T06:05:37.163014Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### model_1 : CONV3D Model","metadata":{"id":"IM4L6jQIeDj3"}},{"cell_type":"code","source":"clear_session()\n#write your model here\nmodel_1 = Sequential()\nmodel_1.add(Conv3D(32, 3, activation = \"elu\", input_shape = (len(img_idx), 120, 120, 3), padding = \"same\"))\nmodel_1.add(MaxPooling3D(padding = \"same\"))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Conv3D(64, 3, activation = \"elu\", padding = \"same\"))\nmodel_1.add(MaxPooling3D(padding = \"same\"))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Conv3D(128, 3, activation = \"elu\"))\nmodel_1.add(MaxPooling3D())\nmodel_1.add(BatchNormalization())\nmodel_1.add(Flatten())\nmodel_1.add(Dense(256, activation = \"elu\"))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Dropout(0.7))\nmodel_1.add(Dense(512, activation = \"elu\"))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Dropout(0.7))\nmodel_1.add(Dense(5, activation = \"softmax\"))\nmodel_1.summary()","metadata":{"id":"rUuKWFn10KDE","outputId":"f164d0da-0a15-496d-a71d-ef8317e1251c","execution":{"iopub.status.busy":"2021-11-11T06:05:37.165120Z","iopub.execute_input":"2021-11-11T06:05:37.166669Z","iopub.status.idle":"2021-11-11T06:05:39.884267Z","shell.execute_reply.started":"2021-11-11T06:05:37.166636Z","shell.execute_reply":"2021-11-11T06:05:39.883397Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Training model_1","metadata":{"id":"e97pSzKgpYCq"}},{"cell_type":"code","source":"model_training(model_1, lr = 0.001, patience = 1, factor = 0.8)","metadata":{"id":"yDfYmX9flAiC","outputId":"0a700367-738a-4ca6-fe6e-94b7174b83e2","execution":{"iopub.status.busy":"2021-11-11T06:05:39.888265Z","iopub.execute_input":"2021-11-11T06:05:39.890056Z","iopub.status.idle":"2021-11-11T07:25:13.549064Z","shell.execute_reply.started":"2021-11-11T06:05:39.889995Z","shell.execute_reply":"2021-11-11T07:25:13.548360Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### model_2 : CNN_GRU Model","metadata":{"id":"ZVdhRnZbeQpL"}},{"cell_type":"code","source":"clear_session()\n\nresnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = (120, 120, 3))\noutput = resnet.layers[-1].output\noutput = Flatten()(output)\n\nresnet = Model(resnet.input, output)\n\nfor layer in resnet.layers :\n  layer.trainable = False\n\nmodel_2 = Sequential()\nmodel_2.add(Input(shape = (len(img_idx), 120, 120, 3)))\nmodel_2.add(TimeDistributed(resnet))\nmodel_2.add(TimeDistributed(BatchNormalization()))\nmodel_2.add(GRU(64, return_sequences = True))\nmodel_2.add(TimeDistributed(BatchNormalization()))\nmodel_2.add(GRU(32))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Dropout(0.5))\nmodel_2.add(Dense(16, activation = \"elu\"))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Dropout(0.5))\nmodel_2.add(Dense(5, activation = \"softmax\"))\nmodel_2.summary()","metadata":{"id":"zwyCegFi0KDM","outputId":"0bd99b19-1721-48d9-b727-70476a2928b4","execution":{"iopub.status.busy":"2021-11-11T07:25:13.551813Z","iopub.execute_input":"2021-11-11T07:25:13.552075Z","iopub.status.idle":"2021-11-11T07:25:16.664182Z","shell.execute_reply.started":"2021-11-11T07:25:13.552042Z","shell.execute_reply":"2021-11-11T07:25:16.663416Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Training model_2","metadata":{"id":"K3JalaiiHAz-"}},{"cell_type":"code","source":"model_training(model_2, lr = 0.001, patience = 1, factor = 0.8)","metadata":{"id":"KnENKje2lOnz","outputId":"fc59e0e1-4ecf-48bf-a20c-7033fd826b3b","execution":{"iopub.status.busy":"2021-11-11T07:25:16.665314Z","iopub.execute_input":"2021-11-11T07:25:16.665550Z","iopub.status.idle":"2021-11-11T08:44:53.893576Z","shell.execute_reply.started":"2021-11-11T07:25:16.665515Z","shell.execute_reply":"2021-11-11T08:44:53.892631Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"\n\n*   model_1 and model_2 have similar validation accuracies\n*   model_2 has around 5 times the no of parameters as model_1\n\nSince the model has to be deployed in a real world scenario, it is better to go with model_1(CONV3D model) which has a considerably lower no of parameters","metadata":{"id":"ur_4O6F-IOg5"}},{"cell_type":"code","source":"","metadata":{"id":"zEzMXPatBl5r"},"execution_count":null,"outputs":[]}]}